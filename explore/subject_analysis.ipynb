{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore Subject Heading Usage**\n",
    "\n",
    "This notebook outlines the process of exploring the use of subject headings by a hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data and Basic Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark(app_name, master_config):\n",
    "    \"\"\"\n",
    "    :params app_name: Name of the app\n",
    "    :params master_config: eg. local[4]\n",
    "    :returns SparkContext, SQLContext, SparkSession:\n",
    "    \"\"\"\n",
    "    conf = (SparkConf().setAppName(app_name).setMaster(master_config))\n",
    "\n",
    "    sc = SparkContext(conf=conf)\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    sql_ctx = SQLContext(sc)\n",
    "    spark = SparkSession(sc)\n",
    "\n",
    "    return (sc, sql_ctx, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc, sql_ctx, spark = init_spark(\"App_name\", \"local[8]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Hub Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and select columns\n",
    "df = spark.read.parquet('/home/tim/Projects/dpla_hub_data/hub_parquet/ARTstor.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Item Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128098"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find total number of items\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Table with Items that have Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET COUNT OF ITEMS WITH NO SUBJECT\n",
    "dfsubjectisnull = df.select('id', 'name', 'subject', isnull(\"subject\") \\\n",
    "  .alias('subjectisnull'))\n",
    "dfsubjectisnull = dfsubjectisnull \\\n",
    "  .filter(dfsubjectisnull[\"subjectisnull\"] == True) \\\n",
    "  .select('id', 'name', 'subject', 'subjectisnull')\n",
    "result0 = dfsubjectisnull.count()\n",
    "\n",
    "# drop null value subject\n",
    "dfSubjectNotNull = df.na.drop(subset=['subject'])\n",
    "\n",
    "# find the number of items that have nth number of terms\n",
    "result1 = dfSubjectNotNull.filter(size('subject') == 1).count()\n",
    "result2 = dfSubjectNotNull.filter(size('subject') == 2).count()\n",
    "result3 = dfSubjectNotNull.filter(size('subject') == 3).count()\n",
    "result4 = dfSubjectNotNull.filter(size('subject') == 4).count()\n",
    "result5 = dfSubjectNotNull.filter(size('subject') == 5).count()\n",
    "result6 = dfSubjectNotNull.filter(size('subject') == 6).count()\n",
    "result7 = dfSubjectNotNull.filter(size('subject') == 7).count()\n",
    "result8 = dfSubjectNotNull.filter(size('subject') == 8).count()\n",
    "result9 = dfSubjectNotNull.filter(size('subject') == 9).count()\n",
    "result10 = dfSubjectNotNull.filter(size('subject') >= 10).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106568"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of items with subjects\n",
    "dfSubjectNotNull.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|Number of Terms|Number of Items|\n",
      "+---------------+---------------+\n",
      "|              0|          21530|\n",
      "|              1|          65608|\n",
      "|              2|           8080|\n",
      "|              3|           9158|\n",
      "|              4|           6893|\n",
      "|              5|           3200|\n",
      "|              6|           2780|\n",
      "|              7|           2605|\n",
      "|              8|           2051|\n",
      "|              9|           1511|\n",
      "|            10+|           4680|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create table of results\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "columns = ['Number of Terms', 'Number of Items']\n",
    "vals = [    \n",
    "\t ('0', result0),\n",
    "\t ('1', result1),\n",
    "\t ('2', result2),\n",
    "\t ('3', result3),\n",
    "\t ('4', result4),\n",
    "\t ('5', result5),\n",
    "\t ('6', result6),\n",
    "\t ('7', result7),\n",
    "\t ('8', result8),\n",
    "\t ('9', result9),\n",
    "\t ('10+', result10)\n",
    "]\n",
    "myresults = spark.createDataFrame(vals, columns)\n",
    "myresults.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploded and Flattened Subjects Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode subject, and select other columns\n",
    "dfIndivSub = dfSubjectNotNull.select(\n",
    "  dfSubjectNotNull.id,\n",
    "  dfSubjectNotNull.name.alias('provider'),\n",
    "  dfSubjectNotNull.intermediateProvider,\n",
    "  dfSubjectNotNull.dataProvider,\n",
    "  dfSubjectNotNull.subject,\n",
    "  explode(dfSubjectNotNull.subject) \\\n",
    "  .alias('subjectHeading')).drop(dfSubjectNotNull.subject)\n",
    "\n",
    "# flatten schema\n",
    "dfIndivSub = dfIndivSub.select(dfIndivSub.subjectHeading.cast(StringType()) \\\n",
    "  .alias('subject'), 'id', 'provider', 'intermediateProvider', 'dataProvider')\n",
    "\n",
    "# add format type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directly compare controlled vocab exact matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+--------------+\n",
      "|             vocab|term_instances|distinct_terms|\n",
      "+------------------+--------------+--------------+\n",
      "|          Original|        279511|         25739|\n",
      "|             LCNAF|         29987|          1597|\n",
      "|              LCSH|        120141|          3231|\n",
      "|               TGM|         86769|          2006|\n",
      "|               AAT|         13653|           408|\n",
      "|FAST Chronological|             0|             0|\n",
      "|    FAST Corporate|          2113|           197|\n",
      "|        FAST Event|            11|             6|\n",
      "|   FAST Form Genre|          7803|           117|\n",
      "|   FAST Geographic|          7633|            74|\n",
      "|     FAST Personal|          3973|           225|\n",
      "|        FAST Title|          2536|            81|\n",
      "|      FAST Topical|        125757|          3546|\n",
      "+------------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load vocabs\n",
    "vocab_lcnaf = spark.read.csv(\n",
    "  \"/home/tim/Projects/subject-term-analysis/vocab/lcnaf.csv\",\n",
    "  multiLine=True, header=True)\n",
    "vocab_lcsh = spark.read.csv(\n",
    "  \"/home/tim/Projects/subject-term-analysis/vocab/lcshTerms.csv\",\n",
    "  multiLine=True, header=True)\n",
    "vocab_tgm = spark.read.csv(\n",
    "  \"/home/tim/Projects/subject-term-analysis/vocab/tgmTerms.csv\",\n",
    "  multiLine=True, header=True)\n",
    "vocab_aat = spark.read.csv(\n",
    "  \"/home/tim/Projects/subject-term-analysis/vocab/aatTerms.csv\",\n",
    "  multiLine=True, header=True)\n",
    "vocab_FASTChronological = spark.read.csv(\n",
    "  \"/home/tim/Projects/subject-term-analysis/vocab/FASTChronologicalTerms.csv\",\n",
    "  multiLine=True, header=True)\n",
    "vocab_FASTCorporate = spark.read.csv(\n",
    "  \"/home/tim/Projects/subject-term-analysis/vocab/FASTCorporateTerms.csv\",\n",
    "  multiLine=True, header=True)\n",
    "vocab_FASTEvent = spark.read.csv(\n",
    "  \"/home/tim/Projects/subject-term-analysis/vocab/FASTEventTerms.csv\",\n",
    "  multiLine=True, header=True)\n",
    "vocab_FASTFormGenre = spark.read.csv(\n",
    "  \"/home/tim/Projects/subject-term-analysis/vocab/FASTFormGenreTerms.csv\",\n",
    "  multiLine=True, header=True)\n",
    "vocab_FASTGeographic = spark.read.csv(\n",
    "  \"/home/tim/Projects/subject-term-analysis/vocab/FASTGeographicTerms.csv\",\n",
    "  multiLine=True, header=True)\n",
    "vocab_tgm = spark.read.csv(\n",
    "  \"/home/tim/Projects/subject-term-analysis/vocab/tgmTerms.csv\",\n",
    "  multiLine=True, header=True)\n",
    "vocab_FASTPersonal = spark.read.csv(\n",
    "  \"/home/tim/Projects/subject-term-analysis/vocab/FASTPersonalTerms.csv\",\n",
    "  multiLine=True, header=True)\n",
    "vocab_FASTTitle = spark.read.csv(\n",
    "  \"/home/tim/Projects/subject-term-analysis/vocab/FASTTitleTerms.csv\",\n",
    "  multiLine=True, header=True)\n",
    "vocab_FASTTopical = spark.read.csv(\n",
    "  \"/home/tim/Projects/subject-term-analysis/vocab/FASTTopicalTerms.csv\",\n",
    "  multiLine=True, header=True)\n",
    "\n",
    "# Inner join LCNAF\n",
    "inner_join_lcnaf = dfIndivSub.join(vocab_lcnaf, \n",
    "  dfIndivSub.subject == vocab_lcnaf.term)\n",
    "lcnaf_count = inner_join_lcnaf.count()\n",
    "distinct_terms_lcnaf = inner_join_lcnaf.select('term').dropDuplicates() \n",
    "lcnaf_count_distinct = distinct_terms_lcnaf.count()\n",
    "\n",
    "# Inner join LCSH\n",
    "inner_join_lcsh = dfIndivSub.join(vocab_lcsh, \n",
    "  dfIndivSub.subject == vocab_lcsh.term)\n",
    "lcsh_count = inner_join_lcsh.count()\n",
    "distinct_terms_lcsh = inner_join_lcsh.select('term').dropDuplicates() \n",
    "lcsh_count_distinct = distinct_terms_lcsh.count()\n",
    "\n",
    "# Inner join TGM\n",
    "inner_join_tgm = dfIndivSub.join(vocab_tgm, \n",
    "  dfIndivSub.subject == vocab_tgm.term)\n",
    "tgm_count = inner_join_tgm.count()\n",
    "distinct_terms_tgm = inner_join_tgm.select('term').dropDuplicates() \n",
    "tgm_count_distinct = distinct_terms_tgm.count()\n",
    "\n",
    "# Inner join AAT\n",
    "inner_join_aat = dfIndivSub.join(vocab_aat, \n",
    "  dfIndivSub.subject == vocab_aat.term)\n",
    "aat_count = inner_join_aat.count()\n",
    "distinct_terms_aat = inner_join_aat.select('term').dropDuplicates() \n",
    "aat_count_distinct = distinct_terms_aat.count()\n",
    "\n",
    "# Inner join FAST Chronological\n",
    "inner_join_FASTChronological = dfIndivSub.join(vocab_FASTChronological,\n",
    "  dfIndivSub.subject == vocab_FASTChronological.term)\n",
    "FASTChronological_count = inner_join_FASTChronological.count()\n",
    "distinct_terms_FASTChronological = inner_join_FASTChronological.select('term').dropDuplicates() \n",
    "FASTChronological_count_distinct = distinct_terms_FASTChronological.count()\n",
    "\n",
    "# Inner join FAST Corporate\n",
    "inner_join_FASTCorporate = dfIndivSub.join(vocab_FASTCorporate, \n",
    "  dfIndivSub.subject == vocab_FASTCorporate.term)\n",
    "FASTCorporate_count = inner_join_FASTCorporate.count()\n",
    "distinct_terms_FASTCorporate = inner_join_FASTCorporate.select('term').dropDuplicates() \n",
    "FASTCorporate_count_distinct = distinct_terms_FASTCorporate.count()\n",
    "\n",
    "# Inner join FAST Event\n",
    "inner_join_FASTEvent = dfIndivSub.join(vocab_FASTEvent, \n",
    "  dfIndivSub.subject == vocab_FASTEvent.term)\n",
    "FASTEvent_count = inner_join_FASTEvent.count()\n",
    "distinct_terms_FASTEvent = inner_join_FASTEvent.select('term').dropDuplicates() \n",
    "FASTEvent_count_distinct = distinct_terms_FASTEvent.count()\n",
    "\n",
    "# Inner join FAST Form Genre\n",
    "inner_join_FASTFormGenre = dfIndivSub.join(vocab_FASTFormGenre, \n",
    "  dfIndivSub.subject == vocab_FASTFormGenre.term)\n",
    "FASTFormGenre_count = inner_join_FASTFormGenre.count()\n",
    "distinct_terms_FASTFormGenre = inner_join_FASTFormGenre.select('term').dropDuplicates() \n",
    "FASTFormGenre_count_distinct = distinct_terms_FASTFormGenre.count()\n",
    "\n",
    "# Inner join FAST Geographic\n",
    "inner_join_FASTGeographic = dfIndivSub.join(vocab_FASTGeographic,\n",
    "  dfIndivSub.subject == vocab_FASTGeographic.term)\n",
    "FASTGeographic_count = inner_join_FASTGeographic.count()\n",
    "distinct_terms_FASTGeographic = inner_join_FASTGeographic.select('term').dropDuplicates() \n",
    "FASTGeographic_count_distinct = distinct_terms_FASTGeographic.count()\n",
    "\n",
    "# Inner join FAST Personal\n",
    "inner_join_FASTPersonal = dfIndivSub.join(vocab_FASTPersonal, \n",
    "  dfIndivSub.subject == vocab_FASTPersonal.term)\n",
    "FASTPersonal_count = inner_join_FASTPersonal.count()\n",
    "distinct_terms_FASTPersonal = inner_join_FASTPersonal.select('term').dropDuplicates() \n",
    "FASTPersonal_count_distinct = distinct_terms_FASTPersonal.count()\n",
    "\n",
    "# Inner join FAST Title\n",
    "inner_join_FASTTitle = dfIndivSub.join(vocab_FASTTitle, \n",
    "  dfIndivSub.subject == vocab_FASTTitle.term)\n",
    "FASTTitle_count = inner_join_FASTTitle.count()\n",
    "distinct_terms_FASTTitle = inner_join_FASTTitle.select('term').dropDuplicates() \n",
    "FASTTitle_count_distinct = distinct_terms_FASTTitle.count()\n",
    "\n",
    "# Inner join FAST Topical\n",
    "inner_join_FASTTopical = dfIndivSub.join(vocab_FASTTopical,\n",
    "  dfIndivSub.subject == vocab_FASTTopical.term)\n",
    "FASTTopical_count = inner_join_FASTTopical.count()\n",
    "distinct_terms_FASTTopical = inner_join_FASTTopical.select('term').dropDuplicates() \n",
    "FASTTopical_count_distinct = distinct_terms_FASTTopical.count()\n",
    "\n",
    "# find original term_instances and distinct_terms count\n",
    "dfIndivSub_count = dfIndivSub.count()\n",
    "dfIndivSub_count_distinct = dfIndivSub.select('subject').dropDuplicates().count()\n",
    "\n",
    "# make a table with results\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "columns = ['vocab', 'term_instances', 'distinct_terms']\n",
    "vals = [    \n",
    "\t ('Original', dfIndivSub_count, dfIndivSub_count_distinct),\n",
    "\t ('LCNAF', lcnaf_count, lcnaf_count_distinct),\n",
    "\t ('LCSH', lcsh_count, lcsh_count_distinct),\n",
    "\t ('TGM', tgm_count, tgm_count_distinct),\n",
    "\t ('AAT', aat_count, aat_count_distinct),\n",
    "\t ('FAST Chronological', FASTChronological_count, FASTChronological_count_distinct),\n",
    "\t ('FAST Corporate', FASTCorporate_count, FASTCorporate_count_distinct),\n",
    "\t ('FAST Event', FASTEvent_count, FASTEvent_count_distinct),\n",
    "\t ('FAST Form Genre', FASTFormGenre_count, FASTFormGenre_count_distinct),\n",
    "\t ('FAST Geographic', FASTGeographic_count, FASTGeographic_count_distinct),\n",
    "\t ('FAST Personal', FASTPersonal_count, FASTPersonal_count_distinct),\n",
    "\t ('FAST Title', FASTTitle_count, FASTTitle_count_distinct),\n",
    "\t ('FAST Topical', FASTTopical_count, FASTTopical_count_distinct)\n",
    "]\n",
    "results_direct_compare = spark.createDataFrame(vals, columns)\n",
    "results_direct_compare.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stepping through controlled vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exact matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+--------------+\n",
      "|            stage|term_instances|distinct_terms|\n",
      "+-----------------+--------------+--------------+\n",
      "|         Original|        279511|         25739|\n",
      "|    Exact Matches|        487145|          5863|\n",
      "|    Complex Terms|         14035|          1335|\n",
      "|Mystery Leftovers|        116935|         18541|\n",
      "+-----------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create one big df of control vocab\n",
    "\n",
    "def union_all(dfs):\n",
    "    if len(dfs) > 1:\n",
    "        return dfs[0].unionAll(union_all(dfs[1:]))\n",
    "    else:\n",
    "        return dfs[0]\n",
    "\n",
    "vocab_all = union_all([vocab_lcnaf, \n",
    "  vocab_lcsh,\n",
    "  vocab_tgm,\n",
    "  vocab_aat,\n",
    "  vocab_FASTChronological,\n",
    "  vocab_FASTCorporate,\n",
    "  vocab_FASTEvent,\n",
    "  vocab_FASTFormGenre,\n",
    "  vocab_FASTGeographic,\n",
    "  vocab_tgm,\n",
    "  vocab_FASTPersonal,\n",
    "  vocab_FASTTitle,\n",
    "  vocab_FASTTopical])\n",
    "\n",
    "\n",
    "# inner join to find exact_match\n",
    "exact_match = dfIndivSub.join(vocab_all, dfIndivSub.subject == vocab_all.term)\n",
    "\n",
    "# get some counts\n",
    "exact_match_count = exact_match.select('subject').count()\n",
    "exact_match_distinct = exact_match.select('term').dropDuplicates() \n",
    "exact_match_distinct_count = exact_match_distinct.count()\n",
    "\n",
    "\n",
    "# subtract to find not_exact_match\n",
    "not_exact_match = dfIndivSub.select('subject', 'id', 'provider',\n",
    "  'intermediateProvider', 'dataProvider') \\\n",
    "  .subtract(exact_match.select('subject', 'id', 'provider', \n",
    "  'intermediateProvider', 'dataProvider'))\n",
    "\n",
    "# find Complex Terms\n",
    "# find subjects that contain LoC double dash style terms \n",
    "expr = '\\-\\-'\n",
    "complex_match = not_exact_match.filter(not_exact_match['subject'].rlike(expr))\n",
    "\n",
    "# count Complex Terms\n",
    "complex_match_count = complex_match.select('subject').count()\n",
    "complex_match_distinct = complex_match.select('subject').dropDuplicates()\n",
    "complex_match_distinct_count = complex_match_distinct.count()\n",
    "\n",
    "# create df of mystery leftovers\n",
    "mystery_leftover = not_exact_match.select('subject', 'id', 'provider',\n",
    "  'intermediateProvider', 'dataProvider') \\\n",
    "  .subtract(complex_match.select('subject', 'id', 'provider', \n",
    "  'intermediateProvider', 'dataProvider'))\n",
    "\n",
    "# count mystery leftovers\n",
    "mystery_leftover_count = mystery_leftover.select('subject').count()\n",
    "mystery_leftover_distinct_count = mystery_leftover.select('subject') \\\n",
    "  .dropDuplicates().count()\n",
    "\n",
    "# create a table\n",
    "columns = ['stage', 'term_instances', 'distinct_terms']\n",
    "vals = [    \n",
    "\t ('Original', dfIndivSub_count, dfIndivSub_count_distinct),\n",
    "\t ('Exact Matches', exact_match_count, exact_match_distinct_count),\n",
    "\t ('Complex Terms', complex_match_count, complex_match_distinct_count),\n",
    "\t ('Mystery Leftovers', mystery_leftover_count,\n",
    "           mystery_leftover_distinct_count),\n",
    "]\n",
    "results_chain_compare = spark.createDataFrame(vals, columns)\n",
    "results_chain_compare.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select complex style terms and break into single terms match with LCSH and LCNAF complex terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------+\n",
      "|               stage|term_instances|distinct_terms|\n",
      "+--------------------+--------------+--------------+\n",
      "|       Complex Terms|         14035|          1335|\n",
      "|Split into Single...|         32274|          1538|\n",
      "|Exact Matches Single|         69894|           118|\n",
      "|Mystery Leftovers...|         27524|          1420|\n",
      "+--------------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split complext terms into single terms\n",
    "single = complex_match.withColumn('subject',explode(split('subject','--')))\n",
    "\n",
    "# count single terms\n",
    "single_count = single.select('subject').count()\n",
    "single_distinct = single.select('subject').dropDuplicates()\n",
    "single_distinct_count = single_distinct.count()\n",
    "\n",
    "# match single terms to lc vocab\n",
    "vocab_lc = union_all([vocab_lcnaf, vocab_lcsh])\n",
    "single_match = single.join(vocab_lc, single.subject == vocab_all.term)\n",
    "\n",
    "# count single term matches\n",
    "single_match_count = single_match.select('subject').count()\n",
    "single_match_distinct = single_match.select('subject').dropDuplicates()\n",
    "single_match_distinct_count = single_match_distinct.count()\n",
    "\n",
    "# create df of leftover single terms\n",
    "single_leftover = single.select('subject', 'id', 'provider',\n",
    "  'intermediateProvider', 'dataProvider') \\\n",
    "  .subtract(single_match.select('subject', 'id', 'provider', \n",
    "  'intermediateProvider', 'dataProvider'))\n",
    "\n",
    "# count leftover single terms\n",
    "single_leftover_count = single_leftover.select('subject').count()\n",
    "single_leftover_distinct = single_leftover.select('subject').dropDuplicates()\n",
    "single_leftover_distinct_count = single_leftover_distinct.count()\n",
    "\n",
    "# create a table\n",
    "columns = ['stage', 'term_instances', 'distinct_terms']\n",
    "vals = [    \n",
    "\t ('Complex Terms', complex_match_count, complex_match_distinct_count),\n",
    "\t ('Split into Single Terms', single_count, single_distinct_count),\n",
    "\t ('Exact Matches Single', single_match_count, single_match_distinct_count),\n",
    "\t ('Mystery Leftovers Singles', single_leftover_count,\n",
    "          single_leftover_distinct_count),\n",
    "]\n",
    "results_complex_to_single = spark.createDataFrame(vals, columns)\n",
    "results_complex_to_single.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
